{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tradução automática com o modelo encoder-decoder\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\"  width='600' heith='150'   src='images/enc_dec.png'>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Nesse notebook vamos usar um dataset de pares de sentenças em inglês e português para treinar uma versão simples da arquitetura encoder-decoder. Os dados foram retirados do site [Tatoeba](https://tatoeba.org/eng/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook feito para a versão 0.4.1 \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from plots.plots import simple_step_plot\n",
    "import  matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    return '%s' % asMinutes(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulação inicial dos dados\n",
    "\n",
    "Em primeiro lugar, vamos pegar todos os pares se sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = []\n",
    "pt = []\n",
    "\n",
    "with open(\"data/eng@pt.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        break_point = line.find(\"@\")\n",
    "        eng.append(line[:break_point])\n",
    "        pt.append(line[break_point+1:].strip())\n",
    "        \n",
    "eng_pt_pairs = [(e,p) for e,p in zip(eng, pt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos dividir o dataset em treino, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_break = int(len(eng_pt_pairs) * 0.8)\n",
    "test_break = valid_break + int((len(eng_pt_pairs) - valid_break) * 0.5)\n",
    "np.random.shuffle(eng_pt_pairs)\n",
    "\n",
    "\n",
    "train_pairs = eng_pt_pairs[:valid_break]\n",
    "valid_pairs = eng_pt_pairs[valid_break:test_break]\n",
    "test_pairs = eng_pt_pairs[test_break:]\n",
    "\n",
    "print(\"training size = \", len(train_pairs))\n",
    "print(\"valid size = \", len(valid_pairs))\n",
    "print(\"test size = \", len(test_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir a classe Lang para guardar o vocabulário de cada lingua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para deixar o as sentenças padronizadas, vamos fazer usar certas funções de normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII\n",
    "\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"\n",
    "    Lowercase, trim, and remove non-letter characters\n",
    "    \"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r'[+-]?\\d+(?:\\.\\d+)?', \"N\", s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub('[^a-zA-z0-9\\s]', '', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===Exemplo de normalização===\\n\")\n",
    "\n",
    "\n",
    "print(\"===Antes===\")\n",
    "\n",
    "for t in train_pairs[0:4]:\n",
    "    p1, p2 = t[0], t[1]\n",
    "    print(p2)\n",
    "    \n",
    "print(\"\\n===Depois===\")\n",
    "\n",
    "for t in train_pairs[0:4]:\n",
    "    p1, p2 = t[0], t[1]\n",
    "    p1 = normalizeString(p1)\n",
    "    p2 = normalizeString(p2)\n",
    "    print(p2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olhando os dados\n",
    "\n",
    "Estamos lidando com pares de sentença. Podemos ver como é a distribuição dos tamanhos no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs_nor = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in train_pairs]\n",
    "valid_pairs_nor = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in valid_pairs]\n",
    "test_pairs_nor = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in test_pairs]\n",
    "\n",
    "all_text_pairs_en = [s1 for s1,_ in train_pairs_nor] + [s1 for s1,_ in valid_pairs_nor] + [s1 for s1,_ in valid_pairs_nor]\n",
    "all_text_pairs_pt = [s1 for _,s1 in train_pairs_nor] + [s1 for _,s1 in valid_pairs_nor] + [s1 for _,s1 in valid_pairs_nor]\n",
    "all_text_pairs = list(zip(all_text_pairs_en, all_text_pairs_pt))\n",
    "\n",
    "en_sizes  = [len(s.split(\" \")) for s in all_text_pairs_en]\n",
    "pt_sizes  = [len(s.split(\" \")) for s in all_text_pairs_pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tamanho das sentenças em inglês:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(en_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o tamanho das sentenças em português:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pt_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o treinamento ficar mais fácil de ser executado aqui, vamos trabalhar com sentenças contendo no máximo 4 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções abaixo vão servir para escolher as sentenças de acordo com o tamanho máximo e construir os vocabulários das respectivas linguas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, pairs, reverse=False):\n",
    "    \"\"\"\n",
    "    Reverse pairs, make Lang instances\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        pairs = [tuple(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    cond1 = len(p[0].split(' ')) < MAX_LENGTH\n",
    "    cond2 = len(p[1].split(' ')) < MAX_LENGTH \n",
    "    return cond1 and cond2\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, pairs, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, pairs, reverse)\n",
    "    pairs = filterPairs(pairs)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim podemos ter as linguas de entrada e saida (*source and target language*) juntamento com os pares de sentenças normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, train_pairs_nor_f = prepareData(\"eng\",\n",
    "                                      \"pt\",\n",
    "                                       train_pairs_nor)\n",
    "print()\n",
    "\n",
    "input_lang, output_lang, _ = prepareData(\"eng\",\n",
    "                                         \"pt\",\n",
    "                                         all_text_pairs)\n",
    "print()\n",
    "\n",
    "_, _, valid_pairs_nor_f = prepareData(\"eng\",\n",
    "                                      \"pt\",\n",
    "                                      valid_pairs_nor)\n",
    "\n",
    "print()\n",
    "\n",
    "_, _, test_pairs_nor_f = prepareData(\"eng\",\n",
    "                                     \"pt\",\n",
    "                                     test_pairs_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_size = input_lang.n_words\n",
    "pt_size = output_lang.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentences 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = random.choice(train_pairs_nor_f)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexesFromSentence(input_lang,example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexesFromSentence(output_lang, example[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size,\n",
    "                                      hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, self.num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, self.num_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Treinamento\n",
    "\n",
    "Observações sobre o treinamento:\n",
    "\n",
    "- O custo vai ser calculado sobre cada token da linguagem.\n",
    "- Uma das técnicas que podem ser usadas para melhorar o treinamento é aquela chamada de **teacher forcing**: com uma probabilidade $p$ vamos fazer com o que o decoder use as palavras da tradução de referência para ser a entrada do decorer no passo seguinte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor,\n",
    "          target_tensor,\n",
    "          encoder,\n",
    "          decoder,\n",
    "          encoder_optimizer,\n",
    "          decoder_optimizer,\n",
    "          criterion,\n",
    "          max_length,\n",
    "          teacher_forcing_ratio=0.5):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length,\n",
    "                                  encoder.hidden_size,\n",
    "                                  device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True\n",
    "\n",
    "    if not random.random() < teacher_forcing_ratio:\n",
    "        use_teacher_forcing = False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.topk(1)\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def get_loss(input_tensor,\n",
    "             target_tensor,\n",
    "             encoder,\n",
    "             decoder,\n",
    "             criterion,\n",
    "             max_length):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length,\n",
    "                                  encoder.hidden_size,\n",
    "                                  device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        _, topone = decoder_output.topk(1)\n",
    "        decoder_input = topone.squeeze().detach()\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def trainIters(encoder,\n",
    "               decoder,\n",
    "               n_iters,\n",
    "               pairs,\n",
    "               valid_pairs,\n",
    "               encoder_path,\n",
    "               decoder_path,\n",
    "               batch_size=32,\n",
    "               status_every=100,\n",
    "               learning_rate=0.01,\n",
    "               teacher_forcing_ratio=0.5):\n",
    "\n",
    "    plot_losses = []\n",
    "    old = 0\n",
    "    start = time.time()\n",
    "    all_loss = []\n",
    "    valid_loss = float(\"inf\")\n",
    "    \n",
    "    \n",
    "\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "\n",
    "    for i, t in enumerate(training_pairs):\n",
    "        input_sen, output_sen = t\n",
    "        loss = train(input_sen,\n",
    "                     output_sen,\n",
    "                     encoder,\n",
    "                     decoder,\n",
    "                     encoder_optimizer,\n",
    "                     decoder_optimizer,\n",
    "                     criterion,\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        plot_losses.append(loss)\n",
    "\n",
    "        if i % status_every == 0 and i != 0:\n",
    "            valid_batch = [tensorsFromPair(random.choice(valid_pairs))\n",
    "                           for i in range(batch_size)]\n",
    "            batch_loss = 0\n",
    "            for t in valid_batch:\n",
    "                input_sen, output_sen = t\n",
    "                batch_loss += get_loss(input_sen,\n",
    "                                       output_sen,\n",
    "                                       encoder,\n",
    "                                       decoder,\n",
    "                                       criterion,\n",
    "                                       MAX_LENGTH)\n",
    "            current_valid_loss = batch_loss / batch_size\n",
    "            \n",
    "            if current_valid_loss < valid_loss:\n",
    "                valid_loss = current_valid_loss\n",
    "                torch.save(encoder.state_dict(), encoder_path)\n",
    "                torch.save(decoder.state_dict(), decoder_path)\n",
    "            print(\"mean training loss = {:.2f}\".format(np.mean(plot_losses)))\n",
    "            print(\"mean valid loss = {:.2f}\".format(current_valid_loss))\n",
    "            print(\"time in {} steps:\".format(status_every), timeSince(start))\n",
    "            print()\n",
    "            all_loss += plot_losses\n",
    "            plot_losses = []\n",
    "            old = i\n",
    "            start = time.time()\n",
    "    \n",
    "    simple_step_plot([all_loss],\n",
    "                     \"loss\",\n",
    "                     \"loss over training\" ,\n",
    "                     \"loss_example.png\",\n",
    "                     figsize=(15, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traduzindo com um modelo\n",
    "\n",
    "Funções de tradução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder,\n",
    "              decoder,\n",
    "              sentence,\n",
    "              max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.data.topk(1)\n",
    "            if topone.item() == EOS_token:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topone.item()])\n",
    "\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "\n",
    "        return \" \".join(decoded_words)\n",
    "    \n",
    "def save_translation(pairs, encoder, decoder, max_length, out_path):\n",
    "    with open(out_path, \"w\") as file:\n",
    "        file.write(\"source,candidate,reference,blue,accuracy\\n\")        \n",
    "        for tuple_ in pairs:\n",
    "            source, reference = tuple_\n",
    "            candidate = translate(encoder,\n",
    "                                  decoder,\n",
    "                                  source,\n",
    "                                  max_length=max_length)\n",
    "            chencherry = SmoothingFunction()\n",
    "            blue = sentence_bleu([reference.split(\" \")],\n",
    "                                 candidate.split(\" \"),\n",
    "                                 smoothing_function=chencherry.method1)\n",
    "            if blue >= 0.95:\n",
    "                acc = 1\n",
    "            else:\n",
    "                acc = 0\n",
    "            line = source + \",\"\n",
    "            line += candidate + \",\"\n",
    "            line += reference + \",\"\n",
    "            line += \"{:.3f},\".format(blue)\n",
    "            line += \"{}\\n\".format(acc)\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLUE\n",
    "\n",
    "Uma métrica popular de tradução automática é a **BLUE** (bilingual evaluation understudy). Ela compara os n-gramas do candidato à tradução com uma tradução padrão. Seja $x$ a sentença de entrada, $\\hat{y}$ a tradução que o modelo oferece e $y$ a tradução de referência.\n",
    "\n",
    "Para definir a nota Blue precisamos definir o n-grama de precisão $P_n$:\n",
    "\n",
    "\\begin{equation}\n",
    "P_n = \\frac{\\text{número de } n\\text{-gramas em } \\hat{y} \\text{ e } y}{\\text{número de } n\\text{-gramas acorrendo em } \\hat{y}}\n",
    "\\end{equation}    \n",
    "\n",
    "Também definimos uma penalidade por previdade ($BP$) para traduções que são mais curtas que a referência. Seja $len(a)$ o tamanho da sequencia $a$, definimos $BP$ como:\n",
    "\n",
    "\\begin{equation}\n",
    "BP=\n",
    "\\begin{cases}\n",
    "1 & \\text{if } len(\\hat{y}) > len(y) \\\\\n",
    "\\exp\\left( 1 - \\frac{len(y)}{len(\\hat{y})} \\right) & \\text{caso contrário}\n",
    "\\end{cases}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "Assim, temos que:\n",
    "\n",
    "\\begin{equation}\n",
    "BLEU = BP \\; \\exp \\left(\\frac{1}{N}  \\sum_{n=1}^{N} \\log P_n \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Note que:\n",
    "\n",
    "- Para evitar computar $\\log 0$ todas as precisões são alteradas para que elas sempre apresentem algum valor maior que $0$\n",
    "- Cada $n$-grama do candidato $\\hat{y}$ é usado no máximo uma vez\n",
    "- E como é computacionalmente custoso calcular $n$-gramas com valores altos de $n$, definimos $N=4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos ver como é a tradução de um modelo não treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "encoder = EncoderRNN(eng_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, pt_size)\n",
    "\n",
    "np.random.shuffle(train_pairs_nor_f)\n",
    "\n",
    "\n",
    "for t in train_pairs_nor_f[0:3]:\n",
    "    print(\"input_sentence : \" + t[0])\n",
    "    neural_translation = translate(encoder,\n",
    "                                   decoder,\n",
    "                                   t[0],\n",
    "                                   max_length=MAX_LENGTH)\n",
    "    print(\"neural translation : \" + neural_translation)\n",
    "    reference = t[1]\n",
    "    print(\"reference translation : \" + reference)\n",
    "    reference = reference.split(\" \")\n",
    "    candidate = neural_translation.split(\" \")\n",
    "    chencherry = SmoothingFunction()\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=chencherry.method1)\n",
    "    print(\"blue score = {:.2f}\".format(score))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando alguns modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeiro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "\n",
    "encoder = EncoderRNN(eng_size, hidden_size, num_layers)\n",
    "decoder = DecoderRNN(hidden_size, pt_size, num_layers)\n",
    "\n",
    "trainIters(encoder=encoder,\n",
    "           decoder=decoder,\n",
    "           n_iters=5000,\n",
    "           pairs=train_pairs_nor_f,\n",
    "           valid_pairs=valid_pairs_nor_f,\n",
    "           encoder_path=\"encoder1.pkl\",\n",
    "           decoder_path=\"decoder1.pkl\",\n",
    "           status_every=200,\n",
    "           learning_rate=0.02,\n",
    "           teacher_forcing_ratio=1)\n",
    "\n",
    "\n",
    "np.random.shuffle(train_pairs_nor_f)\n",
    "\n",
    "save_translation(train_pairs_nor_f[0:1000],\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"training1.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.read_csv(\"training1.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over training data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over training data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando o modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "\n",
    "encoder = EncoderRNN(eng_size, hidden_size, num_layers)\n",
    "decoder = DecoderRNN(hidden_size, pt_size, num_layers)\n",
    "\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"encoder1.pkl\"))\n",
    "decoder.load_state_dict(torch.load(\"decoder1.pkl\"))\n",
    "\n",
    "np.random.shuffle(valid_pairs_nor_f)\n",
    "\n",
    "save_translation(valid_pairs_nor_f[0:1000],\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"valid1.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"valid1.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over valid data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over valid data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "num_layers = 3\n",
    "\n",
    "encoder = EncoderRNN(eng_size, hidden_size, num_layers)\n",
    "decoder = DecoderRNN(hidden_size, pt_size, num_layers)\n",
    "\n",
    "trainIters(encoder=encoder,\n",
    "           decoder=decoder,\n",
    "           n_iters=5000,\n",
    "           pairs=train_pairs_nor_f,\n",
    "           valid_pairs=valid_pairs_nor_f,\n",
    "           encoder_path=\"encoder2.pkl\",\n",
    "           decoder_path=\"decoder2.pkl\",\n",
    "           status_every=200,\n",
    "           learning_rate=0.02,\n",
    "           teacher_forcing_ratio=0.2)\n",
    "\n",
    "\n",
    "np.random.shuffle(train_pairs_nor_f)\n",
    "\n",
    "save_translation(train_pairs_nor_f[0:1000],\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"training2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.read_csv(\"training2.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over training data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over training data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando o resultado do segundo treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "num_layers = 3\n",
    "\n",
    "encoder = EncoderRNN(eng_size, hidden_size, num_layers)\n",
    "decoder = DecoderRNN(hidden_size, pt_size, num_layers)\n",
    "\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"encoder2.pkl\"))\n",
    "decoder.load_state_dict(torch.load(\"decoder2.pkl\"))\n",
    "\n",
    "np.random.shuffle(valid_pairs_nor_f)\n",
    "\n",
    "save_translation(valid_pairs_nor_f[0:1000],\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"valid2.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"valid2.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over valid data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over valid data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Próximos passos\n",
    "\n",
    "- Melhora a busca de hiper pâmetros: **tamanho da célula de memória**, **learning rate**, **teaching forcing**.\n",
    "\n",
    "- Tente construir modelos fazendo uso da classe `nn.LSTM`.\n",
    "\n",
    "- Tente treinar os modelos com outros tamanhos máximos de sentenças. O ideal é ter um modelo de tradução para **todos os tamanhos**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
