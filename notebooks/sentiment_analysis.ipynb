{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Sentimento usando uma rede recorrente\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\"  width='400' heith='100'   src='images/rnn_sent.png'>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Nesse notebook vamos usar um dataset sintético para apreender como realizar análise de sentimento usando uma rede recorrente.\n",
    "\n",
    "Se você nunca usou a biblioteca [PyTorch](https://pytorch.org/) antes, vale a pena rever os conceitos principais nesses notebooks:\n",
    "\n",
    "- [PyTorch basico 1](https://github.com/MLIME/MAC0460/blob/master/notebooks/pytorch_basico1.ipynb)\n",
    "\n",
    "- [PyTorch basico 2](https://github.com/MLIME/MAC0460/blob/master/notebooks/pytorch_basico2.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook feito para a versão 0.4.1 \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torchtext import data\n",
    "import torch.optim as optim\n",
    "from plots.plots import plot_confusion_matrix, plot_histogram_from_labels\n",
    "from text_generation.sentiment import generate_sentiment_data\n",
    "\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar um dataset bem simples. São sentenças curtas que atribuem um atributo que pode ser negativo ou positivo a uma pessoa:\n",
    "\n",
    "- **Paula sempre foi curiosa** (positivo)\n",
    "\n",
    "- **Alberto é prepotente** (negativo)\n",
    "\n",
    "Para não ser muito simplista também vamos colocar algumas sentenças que são neutras, as vezes elas não são\n",
    "nem positivas nem negativas, as vezes elas são simplesmente ambíguas:\n",
    "\n",
    "- **Clara nunca foi esmerada** (neutro)\n",
    "\n",
    "- **Amanda é tanto carinhosa quanto frívolae** (neutro)\n",
    "\n",
    "Esse dataset é *sintético*, i.e., ele é gerado automaticamente por meio de um  script. Isso é mais fácil de gerar, mas são dados bem artificiais. Dados reais são mais complexos e possuem maior ruido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentiment_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses dados estão dividos entre **treino** e **teste**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'data/sentiment_train.csv'\n",
    "test_data_path = 'data/sentiment_test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos colocar as legendas nas etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_legend = ['Negative','Positive', 'Neutral']\n",
    "sentiment2int = {'Negative':0, 'Positive':1, 'Neutral':2}\n",
    "int2sentiment = {i[1]: i[0] for i in sentiment2int.items()}\n",
    "train_labels = train_data[\"label\"].values\n",
    "test_labels =  test_data[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para vermos a distribuição das classes, vamos plotar um histograma. Note que aqui as classes estão balanceadas. Num dataset real de análise de sentimento isso geralmente não acontece. Isso fica claro quando analisamos textos políticos, como [*tweets*](https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment), a grande maioria dos exemplos são de tweets com sentimento negativo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_from_labels(train_labels,\n",
    "                           labels_legend,\n",
    "                           \"train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_from_labels(test_labels,\n",
    "                           labels_legend,\n",
    "                           \"test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados\n",
    "\n",
    "Antes de transformar as palavras em índices, vamos **limpar o texto**, i.e. vamos deixar todo o texto em caixa baixa e vamos retirar as pontuações  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pre_process_text_df(data, field=\"text\"):\n",
    "    \"\"\"\n",
    "    process DataFrame in place\n",
    "    \n",
    "    :param data: DatFrame with text\n",
    "    :type data: pd.DataFrame\n",
    "    :param field: text field\n",
    "    :type field: str\n",
    "    \"\"\"\n",
    "    data[field] = data[field].apply(lambda x: x.lower())\n",
    "    data[field] = data[field].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "\n",
    "def simple_pre_process_text(sentence):\n",
    "    \"\"\"\n",
    "    process sentence\n",
    "    \n",
    "    :param sentence: input sentence\n",
    "    :type sentence: str\n",
    "    :return: processed sentence\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = np.random.randint(0, 100, 10)\n",
    "\n",
    "print(\"====Antes do processamento====\\n\")\n",
    "\n",
    "for i in ints:\n",
    "    print(train_data[\"text\"].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pre_process_text_df(train_data)\n",
    "simple_pre_process_text_df(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====Depois do processamento====\\n\")\n",
    "\n",
    "for i in ints:\n",
    "    print(train_data[\"text\"].values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos salvar o resultado desse procesamento em outro csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[[\"text\", \"label\"]]\n",
    "test_data = test_data[[\"text\", \"label\"]]\n",
    "train_data.to_csv('data/sentiment_train_clean.csv', header=False, index=False)\n",
    "test_data.to_csv('data/sentiment_test_clean.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiper parâmetros\n",
    "\n",
    "Para organizar o código vamos criar uma classe para guardar todos os hiper parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNConfig(object):\n",
    "    \"\"\"\n",
    "    Holds RNN model hyperparams.\n",
    "    \n",
    "    :param vocab_size: vocabulary size\n",
    "    :type vocab_size: int\n",
    "    :param batch_size: batch size for training\n",
    "    :type batch_size: int\n",
    "    :param embedding_dim: embedding dimension\n",
    "    :type embedding_dim: int\n",
    "    :param rnn_dim: rnn dimension\n",
    "    :type rnn_dim: int\n",
    "    :param output_dim: output dimension\n",
    "    :type output_dim: int\n",
    "    :param epochs: number of epochs\n",
    "    :type epochs: int\n",
    "    :param learning_rate: learning rate for the optimizer\n",
    "    :type learning_rate: float\n",
    "    :param momentum: momentum param\n",
    "    :type momentum: float\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size=25000,\n",
    "                 batch_size=32,\n",
    "                 embedding_dim=100,\n",
    "                 rnn_dim=256,\n",
    "                 output_dim=3,\n",
    "                 epochs=3,\n",
    "                 learning_rate=0.01,\n",
    "                 momentum=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Get all attributs values.\n",
    "        :return: all hyperparams as a string\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        status = \"vocab_size = {}\\n\".format(self.vocab_size)\n",
    "        status += \"batch_size = {}\\n\".format(self.batch_size)\n",
    "        status += \"embedding_dim = {}\\n\".format(self.embedding_dim)\n",
    "        status += \"rnn_dim = {}\\n\".format(self.rnn_dim)\n",
    "        status += \"output_dim = {}\\n\".format(self.output_dim)\n",
    "        status += \"epochs = {}\\n\".format(self.epochs)\n",
    "        status += \"learning_rate = {}\\n\".format(self.learning_rate)\n",
    "        status += \"momentum = {}\\n\".format(self.momentum)\n",
    "        return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essa classe podemos listar todos os hiper parâmetros que vamos usar no treinamento e na criação dos iteradores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RNNConfig()\n",
    "print(\"====Hiper parâmetros====\\n\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando texto em tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para faciliar a manipulação dos dados vamos usar certas classes do pacote [torchtext](https://torchtext.readthedocs.io/en/latest/index.html). \n",
    "\n",
    "Em particular, vamos usar a classe `data.Field` para guadar todos os dados de texto:\n",
    "\n",
    "> Field class models common text processing datatypes that can be represented\n",
    "by tensors.  It holds a Vocab object that defines the set of possible values\n",
    "for elements of the field and their corresponding numerical representations.\n",
    "The Field object also holds other parameters relating to how a datatype\n",
    "should be numericalized, such as a tokenization method and the kind of\n",
    "Tensor that should be produced.\n",
    "\n",
    "E vamos usar a classe `data.LabelField` para guardar as etiquetas:\n",
    "\n",
    "> A label field is a shallow wrapper around a standard field designed to hold labels\n",
    "for a classification task. Its only use is to set the unk_token and sequential to `None` by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field()\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar um dataset usando a classe `data.TabularDataset` passando como referência o dataset processado que acabamos de gerar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.TabularDataset(path='data/sentiment_train_clean.csv',\n",
    "                            format=\"csv\",\n",
    "                            fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "test = data.TabularDataset(path='data/sentiment_test_clean.csv',\n",
    "                            format=\"csv\",\n",
    "                            fields=[('text', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir o vocabulário agora, i.e., vamos associar cada palavra a um índice. Note que o texto pode ter um grande número de palavras únicas, vamos deixar que o tamanho máximo do vocabulário seja de 25000 palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tamanho do vocabulário:', len(TEXT.vocab))\n",
    "print('número de classes:', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver quais são as 10 palavras mais frequentes que ocorrem nesse corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = TEXT.vocab.freqs.most_common(10)\n",
    "\n",
    "print(\"====palavra (frequência)====\\n\")\n",
    "\n",
    "for tuple_ in top10:\n",
    "    print(\"{} ({})\".format(tuple_[0], tuple_[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na classe `TEXT.vocab` dois métodos são importantes para fazer a tradução de palavras para índices:\n",
    "\n",
    "- `.itos` passa de índices para palavras.\n",
    "- `.stoi` passa de palavras para índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Usando TEXT.vocab.itos\\n\")\n",
    "print(\"====palavra, índice====\\n\")\n",
    "\n",
    "for i, word in enumerate(TEXT.vocab.itos[:10]):\n",
    "    print(word, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que há dois tokens novos: \n",
    "\n",
    "- **$<$unk$>$**: token que simboliza uma palavra que não está no vocabulário (unknown)\n",
    "- **$<$pad$>$**: token que simboliza uma palavra sem nenhum sentido associado. Esse token é usado apenas para preencher certas lacunas (padding) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Usando TEXT.vocab.stoi\\n\")\n",
    "print(\"====palavra, índice====\\n\")\n",
    "\n",
    "example = \"Betina é tanto ponderada quanto indiferente\"\n",
    "example = simple_pre_process_text(example)\n",
    "\n",
    "for word in example.split():\n",
    "    print(word, TEXT.vocab.stoi[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dividir o dataset de treinamento: 80$\\%$ dos dados vão ser usados para o treinamento enquanto que 20$\\%$ dos dados vão ser usados para validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('número de dados de treinamento:', len(train))\n",
    "print('número de dados de validação:', len(valid))\n",
    "print('número de dados de teste:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre que um corpus contem sentenças de tamanhos diferentes. Mas nos vamos passar para o modelo lotes (*batches*) de sentenças de tamanho iguais. Por isso usamos o token $<$pad$>$, para preencher certas lacunas. Por exemplo considere as sentenças:\n",
    "\n",
    "- faustino parece ser inseguro mas eu não sei\n",
    "- helena sempre foi solícita\n",
    "- sidnei era preocupado\n",
    "\n",
    "depois de transformá-las em vetores de índices temos:\n",
    "\n",
    "- [661, 5, 6, 330, 4, 12, 7, 13]\n",
    "- [46, 9, 3, 234]\n",
    "- [760, 8, 232]\n",
    "\n",
    "igualando os tamanhos com o *padding* temos:\n",
    "\n",
    "- [661, 5, 6, 330, 4, 12, 7, 13]\n",
    "- [46, 9, 3, 234, 1, 1, 1, 1]\n",
    "- [760, 8, 232, 1, 1, 1, 1, 1]\n",
    "\n",
    "\n",
    "Note que se houver muita diferença de tamanho entre os exemplos vamos ter exemplos no batch com um número bem grande de padding o que não é muito eficiente, assim vamos usar a classe `data.BucketIterator` que define um iterador colocando sentenças de mesmo tamanho juntas, minimizando assim o uso de padding.\n",
    "\n",
    "Vamos criar a classe `DataHolder` para guardar os dados de treinamento, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHolder():\n",
    "    \"\"\"\n",
    "    Class to store all data using the data.BucketIterator class.\n",
    "    \n",
    "    :param config: hyper params configuration\n",
    "    :type config: RNNConfig\n",
    "    :param train_dataset: dataset of training data\n",
    "    :type train_dataset: torchtext.data.dataset.Dataset\n",
    "    :param test_dataset: dataset of test data\n",
    "    :type test_dataset: torchtext.data.dataset.Dataset\n",
    "    :param valid_dataset: dataset of valid data\n",
    "    :type valid_dataset: torchtext.data.dataset.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 train,\n",
    "                 valid,\n",
    "                 test):        \n",
    "        self.train_iter = data.BucketIterator(train, batch_size=config.batch_size, repeat=False)\n",
    "        self.valid_iter = data.BucketIterator(valid, batch_size=config.batch_size, repeat=False)\n",
    "        self.test_iter = data.BucketIterator(test, batch_size=len(test), repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, todos os dados que vamos precisar vão estar associados ao objeto `sent_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RNNConfig(vocab_size=len(TEXT.vocab), output_dim=len(LABEL.vocab))\n",
    "sent_data = DataHolder(config, train, valid, test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerar *batches* com esse objeto é bem simples. Cada batch tem associado os métodos `.text` e `.label`, contendo o texto e a etiqueta respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(sent_data.train_iter))\n",
    "\n",
    "print(\"batch.text.shape = {}\".format(batch.text.shape))\n",
    "print(\"batch.text.type = {}\\n\".format(batch.text.type()))\n",
    "print(\"batch.label.shape = {}\".format(batch.label.shape))\n",
    "print(\"batch.label.type = {}\\n\".format(batch.label.type()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para vermos os exemplos temos que transpor o batch pois o shape original dele é `[sent len, batch size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = batch.text.transpose(0, 1)\n",
    "\n",
    "print(\"====Exemplos de sentenças no batch de treinamento====\\n\")\n",
    "\n",
    "examples = batch_t[0:5]\n",
    "for example in examples:\n",
    "    for i in example:\n",
    "        print(TEXT.vocab.itos[i], end=\" \")\n",
    "    print()\n",
    "    for i in example:\n",
    "        print(i.item(), end=\" \")\n",
    "        \n",
    "    print()\n",
    "    print('===')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo o modelo\n",
    "\n",
    "Aqui vamos definir uma rede recorrente (RNN) usando a classe `nn.module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for classification.\n",
    "\n",
    "\n",
    "    :param config: hyper params configuration\n",
    "    :type config: RNNConfig\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.rnn = nn.RNN(config.embedding_dim, config.rnn_dim)\n",
    "        self.fc = nn.Linear(config.rnn_dim, config.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes forward pass\n",
    "\n",
    "        :param x: input tensor\n",
    "        :type x: torch.FloatTensor(shape=(sent len, batch sizes))\n",
    "        :return: logits\n",
    "        :rtype: torch.FloatTensor(shape=(batch size, output_dim))\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        #embedded.shape = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output.shape = [sent len, batch size, rnn dim]\n",
    "        #hidden.shape = [1, batch size, rnn dim]\n",
    "        pred_output = self.fc(hidden.squeeze(0)) \n",
    "        return pred_output\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Computes model's prediction\n",
    "\n",
    "        :param x: input tensor\n",
    "        :type x: torch.FloatTensor(shape=(sent len, batch sizes))\n",
    "        :return: model's predictions\n",
    "        :rtype: torch.LongTensor(shape=(batch size, output_dim))\n",
    "        \"\"\"\n",
    "        out = self.forward(x)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        out = softmax(out)\n",
    "        indices = torch.argmax(out, 1)\n",
    "        return indices\n",
    "\n",
    "    def evaluate_bach(self, batch):\n",
    "        \"\"\"\n",
    "        evaluate batch\n",
    "\n",
    "        :param batch: text batch\n",
    "        :type batch: torchtext.data.batch.Batch\n",
    "        :return: accuracy, model's predictions, batch labels\n",
    "        :rtype: float, torch.Tensor(shape=(batch size)), torch.Tensor(shape=(batch size))\n",
    "        \"\"\"\n",
    "        prediction = model.predict(batch.text)\n",
    "        labels = batch.label.type('torch.LongTensor')\n",
    "        correct = torch.sum(torch.eq(prediction, labels)).float()\n",
    "        accuracy = float(correct/labels.shape[0])\n",
    "        return accuracy, prediction, labels\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver como o modelo prediz os dados de validação antes do treinamento usando uma **matriz de confusão**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(config)\n",
    "\n",
    "valid_bach = next(iter(sent_data.valid_iter))\n",
    "acc, pred, labels = model.evaluate_bach(valid_bach)\n",
    "\n",
    "plot_confusion_matrix(truth=labels.numpy(),\n",
    "                      predictions=pred.numpy(),\n",
    "                      save=False,\n",
    "                      path=\"rnn_confusion_matrix.png\",\n",
    "                      classes=labels_legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de treinamento\n",
    "\n",
    "Aqui são as principais funções que são necessárias para se treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,\n",
    "                iterator,\n",
    "                optimizer,\n",
    "                criterion):\n",
    "    \"\"\"\n",
    "    Train a model over one epoch\n",
    "\n",
    "    :param model: RNN classification model\n",
    "    :type model: RNN \n",
    "    :param config: hyper param configuration\n",
    "    :type config: RNNConfig\n",
    "    :param iterator: data iterator\n",
    "    :type iterator: torchtext.data.iterator.BucketIterator\n",
    "    :param optimizer: torch optimazer\n",
    "    :type optimizer: optim.SGD, optim.Adam, ...\n",
    "    :param criterion: torch criterion (in this case CE)\n",
    "    :type criterion: nn.CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(batch.text)\n",
    "        label = batch.label.type(\"torch.LongTensor\")\n",
    "        loss = criterion(logits, label)\n",
    "        acc, _, _ = model.evaluate_bach(batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    mean_loss = epoch_loss / len(iterator)\n",
    "    mean_acc = epoch_acc / len(iterator)\n",
    "        \n",
    "    return mean_loss, mean_acc\n",
    "\n",
    "\n",
    "def get_valid_loss(model, valid_iter, criterion):\n",
    "    \"\"\"\n",
    "    Get loss from valid data\n",
    "\n",
    "    :param model: RNN classification model\n",
    "    :type model: RNN \n",
    "    :param valid_iter: valid data iterator\n",
    "    :type iterator: torchtext.data.iterator.BucketIterator\n",
    "    :type criterion: nn.CrossEntropyLoss\n",
    "    :return: loss\n",
    "    :rtype: torch.Tensor(shape=[])\n",
    "    \"\"\"\n",
    "    batch = next(iter(valid_iter))\n",
    "    model.eval()\n",
    "    logits = model(batch.text)\n",
    "    label = batch.label.type(\"torch.LongTensor\")\n",
    "    loss = criterion(logits, label)\n",
    "    return loss\n",
    "\n",
    "def training_loop_text_classification(model,\n",
    "                                      config,\n",
    "                                      dataholder,\n",
    "                                      model_path,\n",
    "                                      verbose=True):\n",
    "    \"\"\"\n",
    "    Train a model for text classification\n",
    "\n",
    "    :param model: RNN classification model\n",
    "    :type model: RNN\n",
    "    :param config: hyper param configuration\n",
    "    :type config: RNNConfig\n",
    "    :param dataholder: data\n",
    "    :type dataholder: DataHolder\n",
    "    :param model_path: path to save model params\n",
    "    :type model_path: str\n",
    "    :param verbose: param to control print\n",
    "    :type verbose: bool\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=config.learning_rate,\n",
    "                          momentum=config.momentum)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    train_iter = dataholder.train_iter\n",
    "    valid_iter = dataholder.valid_iter\n",
    "    \n",
    "    best_valid_loss = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        print(\"epoch = ({}/{})\".format(epoch + 1, config.epochs))\n",
    "        train_loss, train_acc = train_epoch(model, train_iter, optimizer, criterion)\n",
    "        valid_loss = get_valid_loss(model,valid_iter, criterion)\n",
    "        msg = \"\\ntrain_loss = {:.3f} | valid_loss = {:.3f}\".format(float(train_loss),float(valid_loss))\n",
    "        if float(valid_loss) < best_valid_loss:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_valid_loss = float(valid_loss)\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "            print(\"train_acc = {}\\n\".format(train_acc))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando o treinamento\n",
    "\n",
    "Vamos realizar um treinamento simples. Sempre lembrando que é a classe `RNNConfig` que controla os hiper parâmetros de treinamento.\n",
    "depois de treinado os parâmetros do modelo vão ser salvos no arquivo `rnn.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RNNConfig(vocab_size=len(TEXT.vocab),\n",
    "                   output_dim=len(LABEL.vocab),\n",
    "                   epochs=8,\n",
    "                   learning_rate=0.03,\n",
    "                   embedding_dim=300,\n",
    "                   rnn_dim=356,\n",
    "                   momentum=0.8)\n",
    "\n",
    "print(\"====Treinando com os hiper parâmetros====\\n\")\n",
    "print(config)\n",
    "\n",
    "model = RNN(config)\n",
    "\n",
    "training_loop_text_classification(model,\n",
    "                                  config,\n",
    "                                  sent_data,\n",
    "                                  \"rnn.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de treinado o modelo, vamos carregar os parâmetros salvos e ver como o modelo está predizendo os dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(config)\n",
    "model.load_state_dict(torch.load('rnn.pkl'))\n",
    "_, pred, labels = model.evaluate_bach(valid_bach)\n",
    "\n",
    "plot_confusion_matrix(truth=labels.numpy(),\n",
    "                      predictions=pred.numpy(),\n",
    "                      save=False,\n",
    "                      path=\"rnn_confusion_matrix.png\",\n",
    "                      classes=labels_legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de treinado o modelo podemos ver como ele avalia certos exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = \"Rurique era sensível\" # positivo\n",
    "ex2 = \"Nice sempre foi dissimulada\" # negativo\n",
    "ex3 = \"Betina é tanto ponderada quanto indiferente\" # neutro\n",
    "ex4 = \"Mel parece ser quezilento, mas eu não sei\" # neutro\n",
    "examples = [ex1, ex2, ex3, ex4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(config)\n",
    "model.load_state_dict(torch.load('rnn.pkl'))\n",
    "\n",
    "print(\"====Como o modelo prediz certos exemplos====\\n\")\n",
    "\n",
    "for example in examples:\n",
    "    print()\n",
    "    print(\"exemplo:\", example)\n",
    "    text = simple_pre_process_text(example)\n",
    "    text_as_int = [TEXT.vocab.stoi[word] for word in text.split(\" \")]\n",
    "    text_as_tensor = torch.Tensor(text_as_int).type('torch.LongTensor')\n",
    "    text_as_tensor = text_as_tensor.view((1, text_as_tensor.shape[0]))\n",
    "    text_as_tensor = text_as_tensor.t()  # putting in the format [sent len, batch size]\n",
    "    pred = int(model.predict(text_as_tensor))\n",
    "    print(\"modelo: \", int2sentiment[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Próximos passos\n",
    "\n",
    "O que foi descrito aqui são as etapas mais básicas, ainda tem muita coisa a ser feita. \n",
    "\n",
    "- Não está claro quais são os melhores hiperpâmetros. Vale a pena realizar vários treinamentos e selecionar quais são os modelos com melhor acurácia no dataset de validação. Depois de feito isso o resultado final do treinamento deve ser avaliado no dataset de teste. Use `test_bach = next(iter(sent_data.test_iter))` e gere a matriz de confusão.\n",
    "\n",
    "- Aqui foi usado apenas um modelo composto por uma **RNN**, mas poderia ser usado uma **GRU** e **LSTM**. Tente construir modelos fazendo uso das classes`nn.GRU` e `nn.LSTM`.\n",
    "\n",
    "- Depois de dominado esse exemplo é importante ir para o **mundo real!**. Ache um dataset interessante de análise de sentimentos (tweets sobre políticos, avaliação de produtos, resenhas de filmes, etc.) e tente aplicar essas técnicas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
